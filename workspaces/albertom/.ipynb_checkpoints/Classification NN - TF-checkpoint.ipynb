{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 - Import packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "from matplotlib import collections  as mc\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import f1_score\n",
    "import time\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (20.0, 10) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Load data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 - Train set: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19423, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-18.341700</td>\n",
       "      <td>-1.499321</td>\n",
       "      <td>-1.355711</td>\n",
       "      <td>4.085164</td>\n",
       "      <td>0.507722</td>\n",
       "      <td>1.323943</td>\n",
       "      <td>-1.158738</td>\n",
       "      <td>3.239028</td>\n",
       "      <td>-0.351744</td>\n",
       "      <td>4.684859</td>\n",
       "      <td>-1.288307</td>\n",
       "      <td>-0.614557</td>\n",
       "      <td>-1.519203</td>\n",
       "      <td>1.271893</td>\n",
       "      <td>0.056804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19.524442</td>\n",
       "      <td>-2.030392</td>\n",
       "      <td>-0.674545</td>\n",
       "      <td>-3.829560</td>\n",
       "      <td>0.021374</td>\n",
       "      <td>-3.275420</td>\n",
       "      <td>1.350335</td>\n",
       "      <td>1.898675</td>\n",
       "      <td>0.694699</td>\n",
       "      <td>-2.756291</td>\n",
       "      <td>-0.794836</td>\n",
       "      <td>1.239952</td>\n",
       "      <td>1.987590</td>\n",
       "      <td>1.351630</td>\n",
       "      <td>0.320299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.877070</td>\n",
       "      <td>-5.641901</td>\n",
       "      <td>3.048596</td>\n",
       "      <td>1.115479</td>\n",
       "      <td>0.016561</td>\n",
       "      <td>0.644258</td>\n",
       "      <td>-0.395762</td>\n",
       "      <td>0.024898</td>\n",
       "      <td>0.223658</td>\n",
       "      <td>0.744945</td>\n",
       "      <td>0.004955</td>\n",
       "      <td>-0.120828</td>\n",
       "      <td>-0.338187</td>\n",
       "      <td>0.931008</td>\n",
       "      <td>0.728083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-5.824436</td>\n",
       "      <td>-7.146028</td>\n",
       "      <td>2.308765</td>\n",
       "      <td>1.485002</td>\n",
       "      <td>1.753751</td>\n",
       "      <td>0.034794</td>\n",
       "      <td>0.535366</td>\n",
       "      <td>-0.180122</td>\n",
       "      <td>1.468998</td>\n",
       "      <td>2.604550</td>\n",
       "      <td>0.730246</td>\n",
       "      <td>-0.127975</td>\n",
       "      <td>0.202254</td>\n",
       "      <td>0.150643</td>\n",
       "      <td>0.239629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-5.021571</td>\n",
       "      <td>4.583527</td>\n",
       "      <td>-3.234845</td>\n",
       "      <td>2.983661</td>\n",
       "      <td>-2.991689</td>\n",
       "      <td>-1.813864</td>\n",
       "      <td>1.493669</td>\n",
       "      <td>-2.691452</td>\n",
       "      <td>-2.037845</td>\n",
       "      <td>2.800191</td>\n",
       "      <td>-1.108040</td>\n",
       "      <td>0.335971</td>\n",
       "      <td>-0.956513</td>\n",
       "      <td>0.223005</td>\n",
       "      <td>0.470329</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6  \\\n",
       "0 -18.341700 -1.499321 -1.355711  4.085164  0.507722  1.323943 -1.158738   \n",
       "1  19.524442 -2.030392 -0.674545 -3.829560  0.021374 -3.275420  1.350335   \n",
       "2   0.877070 -5.641901  3.048596  1.115479  0.016561  0.644258 -0.395762   \n",
       "3  -5.824436 -7.146028  2.308765  1.485002  1.753751  0.034794  0.535366   \n",
       "4  -5.021571  4.583527 -3.234845  2.983661 -2.991689 -1.813864  1.493669   \n",
       "\n",
       "          7         8         9        10        11        12        13  \\\n",
       "0  3.239028 -0.351744  4.684859 -1.288307 -0.614557 -1.519203  1.271893   \n",
       "1  1.898675  0.694699 -2.756291 -0.794836  1.239952  1.987590  1.351630   \n",
       "2  0.024898  0.223658  0.744945  0.004955 -0.120828 -0.338187  0.931008   \n",
       "3 -0.180122  1.468998  2.604550  0.730246 -0.127975  0.202254  0.150643   \n",
       "4 -2.691452 -2.037845  2.800191 -1.108040  0.335971 -0.956513  0.223005   \n",
       "\n",
       "         14  \n",
       "0  0.056804  \n",
       "1  0.320299  \n",
       "2  0.728083  \n",
       "3  0.239629  \n",
       "4  0.470329  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data_1 = pd.read_csv(\"../common/albertom/train_test_val_split/X_train_pca.csv\", index_col=\"Unnamed: 0\")\n",
    "# df_data_1 = df_data_1.head(500)\n",
    "print(df_data_1.shape)\n",
    "df_data_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 - Train labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19423, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>18329863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>51718313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>32674991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>46137138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>15314372</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0         1\n",
       "0  0  18329863\n",
       "1  1  51718313\n",
       "2  1  32674991\n",
       "3  1  46137138\n",
       "4  0  15314372"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_target_data_1 = pd.read_csv(\"../common/albertom/train_test_val_split/y_train.csv\", index_col=\"Unnamed: 0\")\n",
    "# df_target_data_1 = df_target_data_1.head(500)\n",
    "print(df_target_data_1.shape)\n",
    "df_target_data_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 - Test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4033, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.718190</td>\n",
       "      <td>-6.316477</td>\n",
       "      <td>-2.083559</td>\n",
       "      <td>0.975329</td>\n",
       "      <td>-0.296123</td>\n",
       "      <td>1.974713</td>\n",
       "      <td>0.379124</td>\n",
       "      <td>-0.007346</td>\n",
       "      <td>1.958610</td>\n",
       "      <td>-1.165543</td>\n",
       "      <td>0.759262</td>\n",
       "      <td>-0.382965</td>\n",
       "      <td>-0.584148</td>\n",
       "      <td>1.553133</td>\n",
       "      <td>0.568045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.020365</td>\n",
       "      <td>-0.602705</td>\n",
       "      <td>9.139554</td>\n",
       "      <td>0.840974</td>\n",
       "      <td>0.546386</td>\n",
       "      <td>-5.230693</td>\n",
       "      <td>-0.084946</td>\n",
       "      <td>-10.231455</td>\n",
       "      <td>-0.852961</td>\n",
       "      <td>0.451180</td>\n",
       "      <td>0.669972</td>\n",
       "      <td>-0.829123</td>\n",
       "      <td>1.024230</td>\n",
       "      <td>0.229725</td>\n",
       "      <td>0.039168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.975737</td>\n",
       "      <td>-3.348945</td>\n",
       "      <td>-0.572407</td>\n",
       "      <td>0.087099</td>\n",
       "      <td>0.788437</td>\n",
       "      <td>0.297061</td>\n",
       "      <td>0.889188</td>\n",
       "      <td>0.581228</td>\n",
       "      <td>-0.122152</td>\n",
       "      <td>-0.471265</td>\n",
       "      <td>0.488114</td>\n",
       "      <td>-0.081485</td>\n",
       "      <td>-0.172004</td>\n",
       "      <td>-0.577762</td>\n",
       "      <td>-0.305729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-22.105474</td>\n",
       "      <td>0.258111</td>\n",
       "      <td>0.720882</td>\n",
       "      <td>-3.436382</td>\n",
       "      <td>-1.524025</td>\n",
       "      <td>0.087214</td>\n",
       "      <td>-0.444141</td>\n",
       "      <td>0.608868</td>\n",
       "      <td>1.049485</td>\n",
       "      <td>0.409755</td>\n",
       "      <td>-0.329483</td>\n",
       "      <td>-0.048181</td>\n",
       "      <td>-0.009138</td>\n",
       "      <td>-0.527190</td>\n",
       "      <td>1.436177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-14.011642</td>\n",
       "      <td>2.771666</td>\n",
       "      <td>-3.670775</td>\n",
       "      <td>-0.453639</td>\n",
       "      <td>-0.303950</td>\n",
       "      <td>-1.095188</td>\n",
       "      <td>-2.580395</td>\n",
       "      <td>-1.656323</td>\n",
       "      <td>-1.115220</td>\n",
       "      <td>0.806741</td>\n",
       "      <td>-0.176124</td>\n",
       "      <td>0.275890</td>\n",
       "      <td>-0.784060</td>\n",
       "      <td>1.083728</td>\n",
       "      <td>-1.022576</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6  \\\n",
       "0  -1.718190 -6.316477 -2.083559  0.975329 -0.296123  1.974713  0.379124   \n",
       "1   0.020365 -0.602705  9.139554  0.840974  0.546386 -5.230693 -0.084946   \n",
       "2   9.975737 -3.348945 -0.572407  0.087099  0.788437  0.297061  0.889188   \n",
       "3 -22.105474  0.258111  0.720882 -3.436382 -1.524025  0.087214 -0.444141   \n",
       "4 -14.011642  2.771666 -3.670775 -0.453639 -0.303950 -1.095188 -2.580395   \n",
       "\n",
       "           7         8         9        10        11        12        13  \\\n",
       "0  -0.007346  1.958610 -1.165543  0.759262 -0.382965 -0.584148  1.553133   \n",
       "1 -10.231455 -0.852961  0.451180  0.669972 -0.829123  1.024230  0.229725   \n",
       "2   0.581228 -0.122152 -0.471265  0.488114 -0.081485 -0.172004 -0.577762   \n",
       "3   0.608868  1.049485  0.409755 -0.329483 -0.048181 -0.009138 -0.527190   \n",
       "4  -1.656323 -1.115220  0.806741 -0.176124  0.275890 -0.784060  1.083728   \n",
       "\n",
       "         14  \n",
       "0  0.568045  \n",
       "1  0.039168  \n",
       "2 -0.305729  \n",
       "3  1.436177  \n",
       "4 -1.022576  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data_2 = pd.read_csv(\"../common/albertom/train_test_val_split/X_test_pca.csv\", index_col=\"Unnamed: 0\")\n",
    "# df_data_2 = df_data_2.head(100)\n",
    "print(df_data_2.shape)\n",
    "df_data_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 - Test labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4033, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10930294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>55016784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>48129536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>37805023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>22798417</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0         1\n",
       "0  1  10930294\n",
       "1  1  55016784\n",
       "2  0  48129536\n",
       "3  1  37805023\n",
       "4  0  22798417"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_target_data_2 = pd.read_csv(\"../common/albertom/train_test_val_split/y_test.csv\", index_col=\"Unnamed: 0\")\n",
    "# df_target_data_2 = df_target_data_2.head(100)\n",
    "print(df_target_data_2.shape)\n",
    "df_target_data_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 - Val set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3428, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.121868</td>\n",
       "      <td>-4.745601</td>\n",
       "      <td>-3.924026</td>\n",
       "      <td>2.321226</td>\n",
       "      <td>1.560214</td>\n",
       "      <td>0.136213</td>\n",
       "      <td>0.058430</td>\n",
       "      <td>-0.207562</td>\n",
       "      <td>0.129703</td>\n",
       "      <td>0.631257</td>\n",
       "      <td>-0.073079</td>\n",
       "      <td>-0.072595</td>\n",
       "      <td>-0.046537</td>\n",
       "      <td>-1.095496</td>\n",
       "      <td>0.020650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-18.892884</td>\n",
       "      <td>1.320438</td>\n",
       "      <td>2.465213</td>\n",
       "      <td>-0.328190</td>\n",
       "      <td>-2.543895</td>\n",
       "      <td>0.728472</td>\n",
       "      <td>-1.207503</td>\n",
       "      <td>1.209443</td>\n",
       "      <td>0.543729</td>\n",
       "      <td>0.205445</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>-0.282376</td>\n",
       "      <td>-0.080303</td>\n",
       "      <td>0.704908</td>\n",
       "      <td>0.989034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.413480</td>\n",
       "      <td>-3.967846</td>\n",
       "      <td>-1.918874</td>\n",
       "      <td>0.249799</td>\n",
       "      <td>0.485784</td>\n",
       "      <td>0.726944</td>\n",
       "      <td>-0.507952</td>\n",
       "      <td>-0.138389</td>\n",
       "      <td>0.109414</td>\n",
       "      <td>0.208978</td>\n",
       "      <td>-0.092068</td>\n",
       "      <td>0.341340</td>\n",
       "      <td>-0.376660</td>\n",
       "      <td>-0.591348</td>\n",
       "      <td>0.472888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-10.221737</td>\n",
       "      <td>-0.903182</td>\n",
       "      <td>-1.343646</td>\n",
       "      <td>1.040087</td>\n",
       "      <td>2.489991</td>\n",
       "      <td>2.670366</td>\n",
       "      <td>-0.744474</td>\n",
       "      <td>2.248467</td>\n",
       "      <td>-0.269105</td>\n",
       "      <td>-0.410593</td>\n",
       "      <td>-1.574585</td>\n",
       "      <td>0.339117</td>\n",
       "      <td>-0.976271</td>\n",
       "      <td>0.889209</td>\n",
       "      <td>1.044498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.601570</td>\n",
       "      <td>0.028557</td>\n",
       "      <td>-10.087533</td>\n",
       "      <td>2.789941</td>\n",
       "      <td>-7.033029</td>\n",
       "      <td>-5.960131</td>\n",
       "      <td>0.690240</td>\n",
       "      <td>-2.786239</td>\n",
       "      <td>2.339409</td>\n",
       "      <td>5.522192</td>\n",
       "      <td>1.373146</td>\n",
       "      <td>0.145313</td>\n",
       "      <td>0.697186</td>\n",
       "      <td>0.479144</td>\n",
       "      <td>1.207805</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1          2         3         4         5         6  \\\n",
       "0   1.121868 -4.745601  -3.924026  2.321226  1.560214  0.136213  0.058430   \n",
       "1 -18.892884  1.320438   2.465213 -0.328190 -2.543895  0.728472 -1.207503   \n",
       "2   8.413480 -3.967846  -1.918874  0.249799  0.485784  0.726944 -0.507952   \n",
       "3 -10.221737 -0.903182  -1.343646  1.040087  2.489991  2.670366 -0.744474   \n",
       "4  -1.601570  0.028557 -10.087533  2.789941 -7.033029 -5.960131  0.690240   \n",
       "\n",
       "          7         8         9        10        11        12        13  \\\n",
       "0 -0.207562  0.129703  0.631257 -0.073079 -0.072595 -0.046537 -1.095496   \n",
       "1  1.209443  0.543729  0.205445  0.680000 -0.282376 -0.080303  0.704908   \n",
       "2 -0.138389  0.109414  0.208978 -0.092068  0.341340 -0.376660 -0.591348   \n",
       "3  2.248467 -0.269105 -0.410593 -1.574585  0.339117 -0.976271  0.889209   \n",
       "4 -2.786239  2.339409  5.522192  1.373146  0.145313  0.697186  0.479144   \n",
       "\n",
       "         14  \n",
       "0  0.020650  \n",
       "1  0.989034  \n",
       "2  0.472888  \n",
       "3  1.044498  \n",
       "4  1.207805  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data_3 = pd.read_csv(\"../common/albertom/train_test_val_split/X_val_pca.csv\", index_col=\"Unnamed: 0\")\n",
    "# df_data_3 = df_data_3.head(100)\n",
    "print(df_data_3.shape)\n",
    "df_data_3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 - Val labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3428, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>37382735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>16291467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>16215767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>39563885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>50610721</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0         1\n",
       "0  0  37382735\n",
       "1  0  16291467\n",
       "2  0  16215767\n",
       "3  0  39563885\n",
       "4  0  50610721"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_target_data_3 = pd.read_csv(\"../common/albertom/train_test_val_split/y_val.csv\", index_col=\"Unnamed: 0\")\n",
    "# df_target_data_3 = df_target_data_3.head(100)\n",
    "print(df_target_data_3.shape)\n",
    "df_target_data_3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Globals:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 - Parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_steps = 5001\n",
    "k_p = 0.99\n",
    "\n",
    "valid_size = df_data_3.shape[0]\n",
    "test_size = df_data_2.shape[0]\n",
    "batch_size = 128\n",
    "\n",
    "distinct_labels = len(df_target_data_1[\"0\"].unique())\n",
    "\n",
    "train_dataset = df_data_1.values\n",
    "val_dataset = df_data_3.values\n",
    "test_dataset = df_data_2.values\n",
    "\n",
    "split_labels = df_target_data_1[\"0\"].values\n",
    "train_labels = (np.arange(distinct_labels) == split_labels[:,None]).astype(np.float32)\n",
    "\n",
    "test_labels = df_target_data_2[\"0\"].values\n",
    "test_labels = (np.arange(distinct_labels) == test_labels[:,None]).astype(np.float32)\n",
    "\n",
    "val_labels = df_target_data_3[\"0\"].values\n",
    "val_labels = (np.arange(distinct_labels) == val_labels[:,None]).astype(np.float32)\n",
    "\n",
    "num_features = train_dataset.shape[1]\n",
    "num_examples = train_dataset.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - Tensorflow graph: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    X = tf.placeholder(tf.float32, shape=(batch_size, num_features))\n",
    "    t = tf.placeholder(tf.int32, shape=(batch_size, distinct_labels))\n",
    "\n",
    "    L2_reg = tf.placeholder(tf.float32, shape=[])\n",
    "    learning_rate = tf.placeholder(tf.float32, shape=[])\n",
    "    \n",
    "    keep_prob = tf.constant(k_p, tf.float32)\n",
    "    \n",
    "    X_val = tf.constant(val_dataset, tf.float32)\n",
    "    X_test = tf.constant(test_dataset, tf.float32)\n",
    "\n",
    "    # Variables.\n",
    "    num_hidden1 = 10\n",
    "    num_hidden2 = 7\n",
    "    \n",
    "    W1 = tf.Variable(tf.truncated_normal([num_features, num_hidden1]) )\n",
    "    b1 = tf.Variable(tf.zeros([num_hidden1]))\n",
    "\n",
    "    W2 = tf.Variable(tf.truncated_normal([num_hidden1, num_hidden2]))\n",
    "    b2 = tf.Variable(tf.zeros([num_hidden2]))\n",
    "    \n",
    "    W3 = tf.Variable(tf.truncated_normal([num_hidden2, distinct_labels]))\n",
    "    b3 = tf.Variable(tf.zeros([distinct_labels]))\n",
    "    \n",
    "    # Training.\n",
    "    h1 = tf.nn.tanh(tf.matmul(X, W1) + b1)\n",
    "    h2 = tf.nn.tanh(tf.matmul(h1, W2) + b2)\n",
    "    logits = tf.matmul(h2, W3) + b3\n",
    "    \n",
    "    # Loss NO reg.\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, t))\n",
    "    regularization = (tf.nn.l2_loss(W1) + tf.nn.l2_loss(b1) + tf.nn.l2_loss(W2) + tf.nn.l2_loss(b2))\n",
    "    loss = loss + L2_reg * regularization\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "    # Predictions for training, validation and test set.\n",
    "    train_predictions = tf.nn.softmax(logits)\n",
    "    \n",
    "    h1_val = tf.nn.tanh(tf.matmul(X_val, W1) + b1)\n",
    "    h2_val = tf.nn.tanh(tf.matmul(h1, W2) + b2)\n",
    "    logits_val = tf.matmul(h2, W3) + b3\n",
    "    val_predictions = tf.nn.softmax(logits_val)\n",
    "    \n",
    "    h1_test = tf.nn.tanh(tf.matmul(X_test, W1) + b1)\n",
    "    h2_test = tf.nn.tanh(tf.matmul(h1, W2) + b2)\n",
    "    logits_test = tf.matmul(h2, W3) + b3\n",
    "    test_predictions = tf.nn.softmax(logits_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 - Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Given a set of model parameters (learning rate, regularization penalty coefficient), find the optimal parameters with cross validation\n",
    "\n",
    "def holdout_validation(set_of_learning_rates, set_of_regs, X_train, y_train):\n",
    "    \n",
    "    # Get train-validation set stratified (keeps the same distribution) splitter\n",
    "    #skf = StratifiedKFold(n_splits=3)\n",
    "    set_of_params = [(x,y) for x in set_of_learning_rates for y in set_of_regs]\n",
    "    \n",
    "    #print(\"Number of folds: \" + str(n_folds))\n",
    "    print(\"Number of parameters combinations: \" + str(len(set_of_params)))\n",
    "    print(\"We will train \" + str(len(set_of_params)) + \" neural networks for this cv task.\")\n",
    "    \n",
    "    f1 = np.zeros(len(set_of_params))\n",
    "\n",
    "    t1= time.time()\n",
    "    for param_idx, param in enumerate(set_of_params):\n",
    "\n",
    "        print(str(param_idx) + \") Combination: [Parameters: \" +  str(param) + \" | Train model]\")\n",
    "        with tf.Session(graph=graph) as session:\n",
    "            tf.global_variables_initializer().run()\n",
    "                \n",
    "            for step in np.arange(num_steps):\n",
    "\n",
    "                offset = (step * batch_size) % (num_examples - batch_size)\n",
    "                X_batch = X_train[offset:(offset + batch_size), :]\n",
    "                t_batch = y_train[offset:(offset + batch_size)]\n",
    "                feed_dict = {\n",
    "                    X : X_batch,\n",
    "                    t : t_batch,\n",
    "                    L2_reg : param[1],\n",
    "                    learning_rate : param[0]\n",
    "                }\n",
    "                _, l, pred_batch = session.run( [optimizer, loss, train_predictions], feed_dict=feed_dict)\n",
    "                \n",
    "                if (step % 500 == 0):\n",
    "                    print(\"> Minibatch loss at step %d: %f\" % (step, l))\n",
    "                    print(\"> Training f1: %f%%\" % score(pred_batch, t_batch))\n",
    "                    print(\"> Validation f1: %f%%\" % score(val_predictions.eval(), val_labels))\n",
    "                \n",
    "            f1_cv = score(val_predictions.eval(), val_labels)\n",
    "            print(\"|> Final Validation f1: %.1f%%\" % f1_cv)\n",
    "            f1[param_idx] = f1[param_idx] + f1_cv\n",
    "    \n",
    "    print('>>>>>> Gridsearch done in '+ str(round((time.time()-t1)/60, 0)) + ' min')\n",
    "    print(\">>>>>> Best [learning rate - f1] = \", set_of_params[np.argmax(f1)][1])\n",
    "    print(\">>>>>> BEST [reg - f1] = \", set_of_params[np.argmax(f1)][0])\n",
    "    print(\">>>>>> SELECTED VALIDATION f1: %f%%\" % np.max(f1))\n",
    "    print(\">>>>>> THE SET OF F1 PER COMBINATION IS: \" + str(f1))\n",
    "    return set_of_params[np.argmax(f1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score(predictions, labels):\n",
    "    return 100*f1_score(np.argmax(predictions, 1), np.argmax(labels, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_results(predictions, reals):\n",
    "\n",
    "    predicted_classes = np.argmax(predictions, 1)\n",
    "    real_classes = np.argmax(reals, 1)\n",
    "    \n",
    "    print(predicted_classes)\n",
    "    print(real_classes)\n",
    "    \n",
    "    # Plot the results\n",
    "    plt.scatter(np.arange(0, len(predicted_classes)), predicted_classes, color='b')\n",
    "    plt.scatter(np.arange(0, len(real_classes)), real_classes, color='r')\n",
    "\n",
    "    lines = []\n",
    "    ax = plt.axes()\n",
    "\n",
    "    for i in range(predicted_classes.shape[0]):\n",
    "        lines.append([(i, predicted_classes[i]), (i, real_classes[i])])\n",
    "\n",
    "    lc = mc.LineCollection(lines, colors=\"g\", linewidths=0.1)\n",
    "\n",
    "    ax.add_collection(lc)\n",
    "    ax.autoscale()\n",
    "    ax.margins(0.1)\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Training: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 - Cross validation for optimal hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holdout validation on TRAINING-SET for [learning rate, L2_reg] ...\n",
      "Number of parameters combinations: 1521\n",
      "We will train 1521 neural networks for this cv task.\n",
      "0) Combination: [Parameters: (0.001, 0.001) | Train model]\n",
      "> Minibatch loss at step 0: 18.675951\n",
      "> Training f1: 36.666667%\n",
      "> Validation f1: 27.638669%\n",
      "> Minibatch loss at step 500: 1.411620\n",
      "> Training f1: 31.578947%\n",
      "> Validation f1: 19.907407%\n",
      "> Minibatch loss at step 1000: 0.694479\n",
      "> Training f1: 51.063830%\n",
      "> Validation f1: 27.669173%\n",
      "> Minibatch loss at step 1500: 0.918291\n",
      "> Training f1: 31.372549%\n",
      "> Validation f1: 29.034874%\n",
      "> Minibatch loss at step 2000: 0.718024\n",
      "> Training f1: 0.000000%\n",
      "> Validation f1: 25.908667%\n",
      "> Minibatch loss at step 2500: 0.821883\n",
      "> Training f1: 26.086957%\n",
      "> Validation f1: 23.412698%\n",
      "> Minibatch loss at step 3000: 0.565424\n",
      "> Training f1: 21.052632%\n",
      "> Validation f1: 19.238901%\n",
      "> Minibatch loss at step 3500: 0.518004\n",
      "> Training f1: 23.076923%\n",
      "> Validation f1: 15.883669%\n",
      "> Minibatch loss at step 4000: 0.691800\n",
      "> Training f1: 11.764706%\n",
      "> Validation f1: 15.973003%\n",
      "> Minibatch loss at step 4500: 0.627215\n",
      "> Training f1: 15.789474%\n",
      "> Validation f1: 13.133641%\n",
      "> Minibatch loss at step 5000: 0.605387\n",
      "> Training f1: 15.789474%\n",
      "> Validation f1: 11.515864%\n",
      "|> Final Validation f1: 11.5%\n",
      "1) Combination: [Parameters: (0.001, 0.002) | Train model]\n",
      "> Minibatch loss at step 0: 13.413936\n",
      "> Training f1: 37.500000%\n",
      "> Validation f1: 31.788618%\n",
      "> Minibatch loss at step 500: 1.651745\n",
      "> Training f1: 37.288136%\n",
      "> Validation f1: 29.331416%\n",
      "> Minibatch loss at step 1000: 0.917073\n",
      "> Training f1: 26.415094%\n",
      "> Validation f1: 30.482115%\n",
      "> Minibatch loss at step 1500: 0.863126\n",
      "> Training f1: 20.833333%\n",
      "> Validation f1: 27.919227%\n",
      "> Minibatch loss at step 2000: 0.735474\n",
      "> Training f1: 6.896552%\n",
      "> Validation f1: 7.380074%\n",
      "> Minibatch loss at step 2500: 0.663771\n",
      "> Training f1: 0.000000%\n",
      "> Validation f1: 2.067183%\n",
      "> Minibatch loss at step 3000: 0.681548\n",
      "> Training f1: 6.060606%\n",
      "> Validation f1: 3.807107%\n",
      "> Minibatch loss at step 3500: 0.626234\n",
      "> Training f1: 9.090909%\n",
      "> Validation f1: 4.539723%\n",
      "> Minibatch loss at step 4000: 0.660699\n",
      "> Training f1: 0.000000%\n",
      "> Validation f1: 1.820546%\n",
      "> Minibatch loss at step 4500: 0.657647\n",
      "> Training f1: 0.000000%\n",
      "> Validation f1: 2.331606%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/albertomariopirovano/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Minibatch loss at step 5000: 0.677468\n",
      "> Training f1: 0.000000%\n",
      "> Validation f1: 2.827763%\n",
      "|> Final Validation f1: 2.8%\n",
      "2) Combination: [Parameters: (0.001, 0.0030000000000000001) | Train model]\n",
      "> Minibatch loss at step 0: 4.573155\n",
      "> Training f1: 39.622642%\n",
      "> Validation f1: 35.365326%\n",
      "> Minibatch loss at step 500: 1.779334\n",
      "> Training f1: 28.571429%\n",
      "> Validation f1: 37.549407%\n",
      "> Minibatch loss at step 1000: 1.052903\n",
      "> Training f1: 18.604651%\n",
      "> Validation f1: 37.302726%\n",
      "> Minibatch loss at step 1500: 0.903789\n",
      "> Training f1: 20.408163%\n",
      "> Validation f1: 35.840000%\n",
      "> Minibatch loss at step 2000: 0.731854\n",
      "> Training f1: 24.390244%\n",
      "> Validation f1: 28.681424%\n",
      "> Minibatch loss at step 2500: 0.712246\n",
      "> Training f1: 21.621622%\n",
      "> Validation f1: 23.309053%\n",
      "> Minibatch loss at step 3000: 0.786811\n",
      "> Training f1: 11.111111%\n",
      "> Validation f1: 20.607375%\n",
      "> Minibatch loss at step 3500: 0.680542\n",
      "> Training f1: 8.695652%\n",
      "> Validation f1: 19.868996%\n",
      "> Minibatch loss at step 4000: 0.742905\n",
      "> Training f1: 0.000000%\n",
      "> Validation f1: 18.563536%\n",
      "> Minibatch loss at step 4500: 0.712342\n",
      "> Training f1: 17.647059%\n",
      "> Validation f1: 16.930023%\n",
      "> Minibatch loss at step 5000: 0.765198\n",
      "> Training f1: 5.263158%\n",
      "> Validation f1: 18.020022%\n",
      "|> Final Validation f1: 18.0%\n",
      "3) Combination: [Parameters: (0.001, 0.0040000000000000001) | Train model]\n",
      "> Minibatch loss at step 0: 10.278666\n",
      "> Training f1: 47.619048%\n",
      "> Validation f1: 36.777920%\n",
      "> Minibatch loss at step 500: 1.621667\n",
      "> Training f1: 22.580645%\n",
      "> Validation f1: 23.446105%\n",
      "> Minibatch loss at step 1000: 1.131371\n",
      "> Training f1: 33.333333%\n",
      "> Validation f1: 23.628692%\n",
      "> Minibatch loss at step 1500: 1.258940\n",
      "> Training f1: 20.833333%\n",
      "> Validation f1: 25.587959%\n",
      "> Minibatch loss at step 2000: 0.876994\n",
      "> Training f1: 25.641026%\n",
      "> Validation f1: 24.901961%\n",
      "> Minibatch loss at step 2500: 1.000766\n",
      "> Training f1: 16.666667%\n",
      "> Validation f1: 18.863880%\n",
      "> Minibatch loss at step 3000: 0.956165\n",
      "> Training f1: 11.428571%\n",
      "> Validation f1: 15.730337%\n",
      "> Minibatch loss at step 3500: 0.823598\n",
      "> Training f1: 8.695652%\n",
      "> Validation f1: 16.294643%\n",
      "> Minibatch loss at step 4000: 0.774261\n",
      "> Training f1: 0.000000%\n",
      "> Validation f1: 11.542992%\n",
      "> Minibatch loss at step 4500: 0.806011\n",
      "> Training f1: 0.000000%\n",
      "> Validation f1: 8.866995%\n",
      "> Minibatch loss at step 5000: 0.779275\n",
      "> Training f1: 20.000000%\n",
      "> Validation f1: 8.588957%\n",
      "|> Final Validation f1: 8.6%\n",
      "4) Combination: [Parameters: (0.001, 0.0050000000000000001) | Train model]\n",
      "> Minibatch loss at step 0: 4.564153\n",
      "> Training f1: 14.285714%\n",
      "> Validation f1: 13.069647%\n",
      "> Minibatch loss at step 500: 1.769964\n",
      "> Training f1: 36.666667%\n",
      "> Validation f1: 19.702893%\n",
      "> Minibatch loss at step 1000: 0.899862\n",
      "> Training f1: 25.000000%\n",
      "> Validation f1: 19.632881%\n",
      "> Minibatch loss at step 1500: 1.336381\n",
      "> Training f1: 19.047619%\n",
      "> Validation f1: 18.839361%\n",
      "> Minibatch loss at step 2000: 1.135113\n",
      "> Training f1: 22.222222%\n",
      "> Validation f1: 15.528531%\n",
      "> Minibatch loss at step 2500: 0.923015\n",
      "> Training f1: 4.347826%\n",
      "> Validation f1: 8.189655%\n",
      "> Minibatch loss at step 3000: 0.814928\n",
      "> Training f1: 13.636364%\n",
      "> Validation f1: 2.884615%\n",
      "> Minibatch loss at step 3500: 0.707344\n",
      "> Training f1: 8.695652%\n",
      "> Validation f1: 0.515464%\n",
      "> Minibatch loss at step 4000: 0.809958\n",
      "> Training f1: 0.000000%\n",
      "> Validation f1: 0.000000%\n",
      "> Minibatch loss at step 4500: 0.805542\n",
      "> Training f1: 0.000000%\n",
      "> Validation f1: 0.000000%\n",
      "> Minibatch loss at step 5000: 0.819547\n",
      "> Training f1: 0.000000%\n",
      "> Validation f1: 0.000000%\n",
      "|> Final Validation f1: 0.0%\n",
      "5) Combination: [Parameters: (0.001, 0.0060000000000000001) | Train model]\n",
      "> Minibatch loss at step 0: 14.577320\n",
      "> Training f1: 51.612903%\n",
      "> Validation f1: 41.221374%\n",
      "> Minibatch loss at step 500: 2.487648\n",
      "> Training f1: 23.529412%\n",
      "> Validation f1: 28.364689%\n",
      "> Minibatch loss at step 1000: 2.277901\n",
      "> Training f1: 21.428571%\n",
      "> Validation f1: 26.319703%\n",
      "> Minibatch loss at step 1500: 1.485362\n",
      "> Training f1: 33.846154%\n",
      "> Validation f1: 23.931624%\n",
      "> Minibatch loss at step 2000: 1.340191\n",
      "> Training f1: 7.407407%\n",
      "> Validation f1: 21.114865%\n",
      "> Minibatch loss at step 2500: 1.039021\n",
      "> Training f1: 20.833333%\n",
      "> Validation f1: 18.047882%\n",
      "> Minibatch loss at step 3000: 0.947116\n",
      "> Training f1: 34.146341%\n",
      "> Validation f1: 11.111111%\n",
      "> Minibatch loss at step 3500: 0.899020\n",
      "> Training f1: 7.692308%\n",
      "> Validation f1: 3.022670%\n",
      "> Minibatch loss at step 4000: 0.918968\n",
      "> Training f1: 6.666667%\n",
      "> Validation f1: 0.780234%\n",
      "> Minibatch loss at step 4500: 0.912122\n",
      "> Training f1: 0.000000%\n",
      "> Validation f1: 0.522193%\n",
      "> Minibatch loss at step 5000: 0.958508\n",
      "> Training f1: 0.000000%\n",
      "> Validation f1: 0.262123%\n",
      "|> Final Validation f1: 0.3%\n",
      "6) Combination: [Parameters: (0.001, 0.0070000000000000001) | Train model]\n",
      "> Minibatch loss at step 0: 11.183042\n",
      "> Training f1: 39.560440%\n",
      "> Validation f1: 26.246472%\n",
      "> Minibatch loss at step 500: 2.088549\n",
      "> Training f1: 31.250000%\n",
      "> Validation f1: 20.429185%\n",
      "> Minibatch loss at step 1000: 1.433193\n",
      "> Training f1: 31.818182%\n",
      "> Validation f1: 19.389587%\n",
      "> Minibatch loss at step 1500: 1.156644\n",
      "> Training f1: 22.641509%\n",
      "> Validation f1: 18.798450%\n",
      "> Minibatch loss at step 2000: 1.074445\n",
      "> Training f1: 5.882353%\n",
      "> Validation f1: 10.058480%\n",
      "> Minibatch loss at step 2500: 0.948513\n",
      "> Training f1: 0.000000%\n",
      "> Validation f1: 2.077922%\n",
      "> Minibatch loss at step 3000: 1.002141\n",
      "> Training f1: 0.000000%\n",
      "> Validation f1: 0.000000%\n",
      "> Minibatch loss at step 3500: 0.895034\n",
      "> Training f1: 0.000000%\n",
      "> Validation f1: 0.000000%\n",
      "> Minibatch loss at step 4000: 0.956762\n",
      "> Training f1: 0.000000%\n",
      "> Validation f1: 0.000000%\n",
      "> Minibatch loss at step 4500: 0.948503\n",
      "> Training f1: 0.000000%\n",
      "> Validation f1: 0.000000%\n",
      "> Minibatch loss at step 5000: 1.002466\n",
      "> Training f1: 0.000000%\n",
      "> Validation f1: 0.000000%\n",
      "|> Final Validation f1: 0.0%\n",
      "7) Combination: [Parameters: (0.001, 0.0080000000000000002) | Train model]\n",
      "> Minibatch loss at step 0: 20.463392\n",
      "> Training f1: 42.105263%\n",
      "> Validation f1: 31.112570%\n",
      "> Minibatch loss at step 500: 4.835849\n",
      "> Training f1: 33.333333%\n",
      "> Validation f1: 20.463847%\n",
      "> Minibatch loss at step 1000: 2.043914\n",
      "> Training f1: 37.735849%\n",
      "> Validation f1: 24.459359%\n",
      "> Minibatch loss at step 1500: 1.463908\n",
      "> Training f1: 31.884058%\n",
      "> Validation f1: 25.871418%\n",
      "> Minibatch loss at step 2000: 1.482348\n",
      "> Training f1: 8.163265%\n",
      "> Validation f1: 23.451693%\n",
      "> Minibatch loss at step 2500: 1.197812\n",
      "> Training f1: 26.415094%\n",
      "> Validation f1: 22.319093%\n",
      "> Minibatch loss at step 3000: 0.955982\n",
      "> Training f1: 36.734694%\n",
      "> Validation f1: 23.519164%\n",
      "> Minibatch loss at step 3500: 0.902433\n",
      "> Training f1: 32.432432%\n",
      "> Validation f1: 22.488479%\n",
      "> Minibatch loss at step 4000: 1.088684\n",
      "> Training f1: 26.666667%\n",
      "> Validation f1: 19.844358%\n",
      "> Minibatch loss at step 4500: 0.905055\n",
      "> Training f1: 19.047619%\n",
      "> Validation f1: 18.811881%\n",
      "> Minibatch loss at step 5000: 0.940348\n",
      "> Training f1: 30.188679%\n",
      "> Validation f1: 19.180819%\n",
      "|> Final Validation f1: 19.2%\n",
      "8) Combination: [Parameters: (0.001, 0.0090000000000000011) | Train model]\n",
      "> Minibatch loss at step 0: 15.575118\n",
      "> Training f1: 27.659574%\n",
      "> Validation f1: 28.464138%\n",
      "> Minibatch loss at step 500: 3.149851\n",
      "> Training f1: 23.529412%\n",
      "> Validation f1: 27.948911%\n",
      "> Minibatch loss at step 1000: 2.010642\n",
      "> Training f1: 26.086957%\n",
      "> Validation f1: 20.938628%\n",
      "> Minibatch loss at step 1500: 1.486071\n",
      "> Training f1: 21.739130%\n",
      "> Validation f1: 15.774648%\n",
      "> Minibatch loss at step 2000: 1.368550\n",
      "> Training f1: 15.384615%\n",
      "> Validation f1: 14.702920%\n",
      "> Minibatch loss at step 2500: 1.174429\n",
      "> Training f1: 10.256410%\n",
      "> Validation f1: 13.390011%\n",
      "> Minibatch loss at step 3000: 1.271922\n",
      "> Training f1: 21.052632%\n",
      "> Validation f1: 8.045977%\n",
      "> Minibatch loss at step 3500: 1.184348\n",
      "> Training f1: 7.407407%\n",
      "> Validation f1: 6.627219%\n",
      "> Minibatch loss at step 4000: 1.251008\n",
      "> Training f1: 10.526316%\n",
      "> Validation f1: 5.818182%\n",
      "> Minibatch loss at step 4500: 1.238661\n",
      "> Training f1: 5.882353%\n",
      "> Validation f1: 3.726708%\n",
      "> Minibatch loss at step 5000: 1.188350\n",
      "> Training f1: 5.405405%\n",
      "> Validation f1: 3.960396%\n",
      "|> Final Validation f1: 4.0%\n",
      "9) Combination: [Parameters: (0.001, 0.010000000000000002) | Train model]\n",
      "> Minibatch loss at step 0: 10.408913\n",
      "> Training f1: 45.614035%\n",
      "> Validation f1: 28.865979%\n",
      "> Minibatch loss at step 500: 2.981705\n",
      "> Training f1: 33.333333%\n",
      "> Validation f1: 21.138211%\n",
      "> Minibatch loss at step 1000: 2.293416\n",
      "> Training f1: 24.489796%\n",
      "> Validation f1: 21.276596%\n",
      "> Minibatch loss at step 1500: 1.853017\n",
      "> Training f1: 21.818182%\n",
      "> Validation f1: 21.273167%\n",
      "> Minibatch loss at step 2000: 1.599925\n",
      "> Training f1: 23.255814%\n",
      "> Validation f1: 21.652893%\n",
      "> Minibatch loss at step 2500: 1.347600\n",
      "> Training f1: 10.000000%\n",
      "> Validation f1: 18.800358%\n",
      "> Minibatch loss at step 3000: 1.212233\n",
      "> Training f1: 5.000000%\n",
      "> Validation f1: 16.550349%\n",
      "> Minibatch loss at step 3500: 0.993736\n",
      "> Training f1: 12.903226%\n",
      "> Validation f1: 10.669694%\n",
      "> Minibatch loss at step 4000: 0.999618\n",
      "> Training f1: 6.451613%\n",
      "> Validation f1: 6.380368%\n",
      "> Minibatch loss at step 4500: 1.000504\n",
      "> Training f1: 17.647059%\n",
      "> Validation f1: 2.816901%\n",
      "> Minibatch loss at step 5000: 1.050006\n",
      "> Training f1: 5.555556%\n",
      "> Validation f1: 1.303781%\n",
      "|> Final Validation f1: 1.3%\n",
      "10) Combination: [Parameters: (0.001, 0.010999999999999999) | Train model]\n",
      "> Minibatch loss at step 0: 9.361970\n",
      "> Training f1: 31.067961%\n",
      "> Validation f1: 30.127941%\n",
      "> Minibatch loss at step 500: 2.285161\n",
      "> Training f1: 20.000000%\n",
      "> Validation f1: 23.012869%\n",
      "> Minibatch loss at step 1000: 1.184161\n",
      "> Training f1: 26.666667%\n",
      "> Validation f1: 12.473573%\n",
      "> Minibatch loss at step 1500: 1.216463\n",
      "> Training f1: 6.060606%\n",
      "> Validation f1: 7.263923%\n",
      "> Minibatch loss at step 2000: 1.160695\n",
      "> Training f1: 6.896552%\n",
      "> Validation f1: 5.479452%\n",
      "> Minibatch loss at step 2500: 1.091110\n",
      "> Training f1: 0.000000%\n",
      "> Validation f1: 2.813299%\n",
      "> Minibatch loss at step 3000: 1.150536\n",
      "> Training f1: 6.060606%\n",
      "> Validation f1: 1.038961%\n",
      "> Minibatch loss at step 3500: 1.064678\n",
      "> Training f1: 0.000000%\n",
      "> Validation f1: 0.783290%\n",
      "> Minibatch loss at step 4000: 1.132663\n",
      "> Training f1: 0.000000%\n",
      "> Validation f1: 2.056555%\n",
      "> Minibatch loss at step 4500: 1.132326\n",
      "> Training f1: 0.000000%\n",
      "> Validation f1: 1.037613%\n",
      "> Minibatch loss at step 5000: 1.142812\n",
      "> Training f1: 0.000000%\n",
      "> Validation f1: 1.293661%\n",
      "|> Final Validation f1: 1.3%\n",
      "11) Combination: [Parameters: (0.001, 0.012) | Train model]\n",
      "> Minibatch loss at step 0: 12.755251\n",
      "> Training f1: 42.276423%\n",
      "> Validation f1: 35.555556%\n",
      "> Minibatch loss at step 500: 3.807976\n",
      "> Training f1: 27.586207%\n",
      "> Validation f1: 30.569948%\n",
      "> Minibatch loss at step 1000: 1.883551\n",
      "> Training f1: 33.962264%\n",
      "> Validation f1: 28.766190%\n",
      "> Minibatch loss at step 1500: 1.608997\n",
      "> Training f1: 25.454545%\n",
      "> Validation f1: 25.574273%\n",
      "> Minibatch loss at step 2000: 1.252870\n",
      "> Training f1: 17.647059%\n",
      "> Validation f1: 11.870101%\n",
      "> Minibatch loss at step 2500: 1.219272\n",
      "> Training f1: 0.000000%\n",
      "> Validation f1: 0.000000%\n",
      "> Minibatch loss at step 3000: 1.231566\n",
      "> Training f1: 0.000000%\n",
      "> Validation f1: 0.000000%\n",
      "> Minibatch loss at step 3500: 1.154942\n",
      "> Training f1: 0.000000%\n",
      "> Validation f1: 0.000000%\n",
      "> Minibatch loss at step 4000: 1.187118\n",
      "> Training f1: 0.000000%\n",
      "> Validation f1: 0.000000%\n",
      "> Minibatch loss at step 4500: 1.213671\n",
      "> Training f1: 0.000000%\n",
      "> Validation f1: 0.000000%\n",
      "> Minibatch loss at step 5000: 1.194571\n",
      "> Training f1: 0.000000%\n",
      "> Validation f1: 0.000000%\n",
      "|> Final Validation f1: 0.0%\n",
      "12) Combination: [Parameters: (0.001, 0.013000000000000001) | Train model]\n",
      "> Minibatch loss at step 0: 22.785988\n",
      "> Training f1: 40.277778%\n",
      "> Validation f1: 34.280792%\n",
      "> Minibatch loss at step 500: 3.103447\n",
      "> Training f1: 29.629630%\n",
      "> Validation f1: 27.734375%\n",
      "> Minibatch loss at step 1000: 1.580931\n",
      "> Training f1: 5.405405%\n",
      "> Validation f1: 18.804921%\n",
      "> Minibatch loss at step 1500: 1.285864\n",
      "> Training f1: 18.604651%\n",
      "> Validation f1: 17.114428%\n",
      "> Minibatch loss at step 2000: 1.263504\n",
      "> Training f1: 6.060606%\n",
      "> Validation f1: 14.348786%\n",
      "> Minibatch loss at step 2500: 1.159093\n",
      "> Training f1: 6.451613%\n",
      "> Validation f1: 7.343941%\n",
      "> Minibatch loss at step 3000: 1.193449\n",
      "> Training f1: 11.111111%\n",
      "> Validation f1: 6.965174%\n",
      "> Minibatch loss at step 3500: 1.114442\n",
      "> Training f1: 0.000000%\n",
      "> Validation f1: 6.716418%\n",
      "> Minibatch loss at step 4000: 1.215002\n",
      "> Training f1: 0.000000%\n",
      "> Validation f1: 4.828463%\n",
      "> Minibatch loss at step 4500: 1.180524\n",
      "> Training f1: 17.647059%\n",
      "> Validation f1: 7.398274%\n",
      "> Minibatch loss at step 5000: 1.173622\n",
      "> Training f1: 0.000000%\n",
      "> Validation f1: 6.724782%\n",
      "|> Final Validation f1: 6.7%\n",
      "13) Combination: [Parameters: (0.001, 0.014000000000000002) | Train model]\n",
      "> Minibatch loss at step 0: 13.233441\n",
      "> Training f1: 0.000000%\n",
      "> Validation f1: 1.265823%\n",
      "> Minibatch loss at step 500: 3.511203\n",
      "> Training f1: 12.765957%\n",
      "> Validation f1: 25.747126%\n",
      "> Minibatch loss at step 1000: 2.035349\n",
      "> Training f1: 7.692308%\n",
      "> Validation f1: 27.423469%\n",
      "> Minibatch loss at step 1500: 1.644952\n",
      "> Training f1: 31.034483%\n",
      "> Validation f1: 26.498003%\n",
      "> Minibatch loss at step 2000: 1.505241\n",
      "> Training f1: 33.962264%\n",
      "> Validation f1: 23.442136%\n",
      "> Minibatch loss at step 2500: 1.426274\n",
      "> Training f1: 25.641026%\n",
      "> Validation f1: 16.650899%\n",
      "> Minibatch loss at step 3000: 1.354644\n",
      "> Training f1: 11.111111%\n",
      "> Validation f1: 9.203143%\n",
      "> Minibatch loss at step 3500: 1.191276\n",
      "> Training f1: 0.000000%\n",
      "> Validation f1: 4.406365%\n",
      "> Minibatch loss at step 4000: 1.230961\n",
      "> Training f1: 0.000000%\n",
      "> Validation f1: 1.285347%\n",
      "> Minibatch loss at step 4500: 1.257479\n",
      "> Training f1: 0.000000%\n",
      "> Validation f1: 0.782269%\n",
      "> Minibatch loss at step 5000: 1.266671\n",
      "> Training f1: 0.000000%\n",
      "> Validation f1: 0.000000%\n",
      "|> Final Validation f1: 0.0%\n",
      "14) Combination: [Parameters: (0.001, 0.014999999999999999) | Train model]\n",
      "> Minibatch loss at step 0: 19.843803\n",
      "> Training f1: 36.800000%\n",
      "> Validation f1: 30.795377%\n",
      "> Minibatch loss at step 500: 2.219415\n",
      "> Training f1: 17.543860%\n",
      "> Validation f1: 24.011299%\n",
      "> Minibatch loss at step 1000: 1.649257\n",
      "> Training f1: 37.209302%\n",
      "> Validation f1: 21.287129%\n",
      "> Minibatch loss at step 1500: 1.714683\n",
      "> Training f1: 16.326531%\n",
      "> Validation f1: 21.645022%\n",
      "> Minibatch loss at step 2000: 1.457959\n",
      "> Training f1: 15.000000%\n",
      "> Validation f1: 15.172414%\n",
      "> Minibatch loss at step 2500: 1.409821\n",
      "> Training f1: 5.882353%\n",
      "> Validation f1: 8.026756%\n",
      "> Minibatch loss at step 3000: 1.325725\n",
      "> Training f1: 11.111111%\n",
      "> Validation f1: 3.398058%\n",
      "> Minibatch loss at step 3500: 1.220550\n",
      "> Training f1: 8.333333%\n",
      "> Validation f1: 1.801802%\n",
      "> Minibatch loss at step 4000: 1.293675\n",
      "> Training f1: 6.666667%\n",
      "> Validation f1: 0.784314%\n",
      "> Minibatch loss at step 4500: 1.273558\n",
      "> Training f1: 0.000000%\n",
      "> Validation f1: 0.000000%\n",
      "> Minibatch loss at step 5000: 1.298888\n",
      "> Training f1: 0.000000%\n",
      "> Validation f1: 0.000000%\n",
      "|> Final Validation f1: 0.0%\n",
      "15) Combination: [Parameters: (0.001, 0.016) | Train model]\n",
      "> Minibatch loss at step 0: 20.654587\n",
      "> Training f1: 45.454545%\n",
      "> Validation f1: 37.150274%\n",
      "> Minibatch loss at step 500: 4.061280\n",
      "> Training f1: 27.586207%\n",
      "> Validation f1: 28.037383%\n",
      "> Minibatch loss at step 1000: 2.055612\n",
      "> Training f1: 44.897959%\n",
      "> Validation f1: 27.395316%\n",
      "> Minibatch loss at step 1500: 1.631735\n",
      "> Training f1: 45.070423%\n",
      "> Validation f1: 24.000000%\n",
      "> Minibatch loss at step 2000: 1.549898\n",
      "> Training f1: 22.727273%\n",
      "> Validation f1: 16.682287%\n",
      "> Minibatch loss at step 2500: 1.373322\n",
      "> Training f1: 5.405405%\n",
      "> Validation f1: 9.163803%\n",
      "> Minibatch loss at step 3000: 1.364826\n",
      "> Training f1: 20.512821%\n",
      "> Validation f1: 2.547771%\n",
      "> Minibatch loss at step 3500: 1.287055\n",
      "> Training f1: 0.000000%\n",
      "> Validation f1: 0.522876%\n",
      "> Minibatch loss at step 4000: 1.311836\n",
      "> Training f1: 0.000000%\n",
      "> Validation f1: 0.523560%\n",
      "> Minibatch loss at step 4500: 1.321301\n",
      "> Training f1: 6.250000%\n",
      "> Validation f1: 0.522876%\n",
      "> Minibatch loss at step 5000: 1.333103\n",
      "> Training f1: 0.000000%\n",
      "> Validation f1: 0.522876%\n",
      "|> Final Validation f1: 0.5%\n",
      "16) Combination: [Parameters: (0.001, 0.017000000000000001) | Train model]\n",
      "> Minibatch loss at step 0: 5.850298\n",
      "> Training f1: 17.910448%\n",
      "> Validation f1: 13.894456%\n",
      "> Minibatch loss at step 500: 1.845813\n",
      "> Training f1: 19.047619%\n",
      "> Validation f1: 12.386157%\n",
      "> Minibatch loss at step 1000: 1.405747\n",
      "> Training f1: 15.384615%\n",
      "> Validation f1: 12.578616%\n",
      "> Minibatch loss at step 1500: 1.363836\n",
      "> Training f1: 6.060606%\n",
      "> Validation f1: 7.598039%\n",
      "> Minibatch loss at step 2000: 1.351850\n",
      "> Training f1: 0.000000%\n",
      "> Validation f1: 1.822917%\n",
      "> Minibatch loss at step 2500: 1.344540\n",
      "> Training f1: 0.000000%\n",
      "> Validation f1: 0.526316%\n",
      "> Minibatch loss at step 3000: 1.305523\n",
      "> Training f1: 0.000000%\n",
      "> Validation f1: 1.047120%\n",
      "> Minibatch loss at step 3500: 1.268260\n",
      "> Training f1: 8.695652%\n",
      "> Validation f1: 0.525624%\n",
      "> Minibatch loss at step 4000: 1.290804\n",
      "> Training f1: 0.000000%\n",
      "> Validation f1: 0.000000%\n",
      "> Minibatch loss at step 4500: 1.290550\n",
      "> Training f1: 0.000000%\n",
      "> Validation f1: 0.000000%\n",
      "> Minibatch loss at step 5000: 1.295028\n",
      "> Training f1: 0.000000%\n",
      "> Validation f1: 0.263158%\n",
      "|> Final Validation f1: 0.3%\n",
      "17) Combination: [Parameters: (0.001, 0.018000000000000002) | Train model]\n",
      "> Minibatch loss at step 0: 15.809042\n",
      "> Training f1: 37.113402%\n",
      "> Validation f1: 23.674912%\n",
      "> Minibatch loss at step 500: 3.144628\n",
      "> Training f1: 41.935484%\n",
      "> Validation f1: 17.065868%\n",
      "> Minibatch loss at step 1000: 2.115456\n",
      "> Training f1: 26.923077%\n",
      "> Validation f1: 15.633423%\n",
      "> Minibatch loss at step 1500: 1.802190\n",
      "> Training f1: 24.489796%\n",
      "> Validation f1: 10.176532%\n",
      "> Minibatch loss at step 2000: 1.738534\n",
      "> Training f1: 0.000000%\n",
      "> Validation f1: 0.258065%\n",
      "> Minibatch loss at step 2500: 1.732543\n",
      "> Training f1: 0.000000%\n",
      "> Validation f1: 0.000000%\n",
      "> Minibatch loss at step 3000: 1.681099\n",
      "> Training f1: 0.000000%\n",
      "> Validation f1: 0.000000%\n",
      "> Minibatch loss at step 3500: 1.595647\n",
      "> Training f1: 0.000000%\n",
      "> Validation f1: 0.000000%\n",
      "> Minibatch loss at step 4000: 1.637913\n",
      "> Training f1: 0.000000%\n",
      "> Validation f1: 0.000000%\n",
      "> Minibatch loss at step 4500: 1.604823\n",
      "> Training f1: 0.000000%\n",
      "> Validation f1: 0.000000%\n",
      "> Minibatch loss at step 5000: 1.638594\n",
      "> Training f1: 0.000000%\n",
      "> Validation f1: 0.000000%\n",
      "|> Final Validation f1: 0.0%\n",
      "18) Combination: [Parameters: (0.001, 0.019000000000000003) | Train model]\n",
      "> Minibatch loss at step 0: 5.308815\n",
      "> Training f1: 44.247788%\n",
      "> Validation f1: 33.486430%\n",
      "> Minibatch loss at step 500: 2.985208\n",
      "> Training f1: 32.258065%\n",
      "> Validation f1: 28.075970%\n",
      "> Minibatch loss at step 1000: 2.204341\n",
      "> Training f1: 28.571429%\n",
      "> Validation f1: 22.641509%\n",
      "> Minibatch loss at step 1500: 1.877494\n",
      "> Training f1: 17.021277%\n",
      "> Validation f1: 13.363029%\n",
      "> Minibatch loss at step 2000: 1.729566\n",
      "> Training f1: 12.903226%\n",
      "> Validation f1: 1.804124%\n",
      "> Minibatch loss at step 2500: 1.692715\n",
      "> Training f1: 6.451613%\n",
      "> Validation f1: 0.262123%\n",
      "> Minibatch loss at step 3000: 1.717502\n",
      "> Training f1: 6.060606%\n",
      "> Validation f1: 0.263158%\n",
      "> Minibatch loss at step 3500: 1.611846\n",
      "> Training f1: 0.000000%\n",
      "> Validation f1: 0.263158%\n",
      "> Minibatch loss at step 4000: 1.677271\n",
      "> Training f1: 0.000000%\n",
      "> Validation f1: 0.000000%\n",
      "> Minibatch loss at step 4500: 1.663998\n",
      "> Training f1: 0.000000%\n",
      "> Validation f1: 0.263505%\n",
      "> Minibatch loss at step 5000: 1.595418\n",
      "> Training f1: 5.882353%\n",
      "> Validation f1: 0.263158%\n",
      "|> Final Validation f1: 0.3%\n",
      "19) Combination: [Parameters: (0.001, 0.02) | Train model]\n",
      "> Minibatch loss at step 0: 8.856177\n",
      "> Training f1: 11.267606%\n",
      "> Validation f1: 25.799215%\n",
      "> Minibatch loss at step 500: 3.071943\n",
      "> Training f1: 20.000000%\n",
      "> Validation f1: 31.586303%\n",
      "> Minibatch loss at step 1000: 2.056737\n",
      "> Training f1: 18.181818%\n",
      "> Validation f1: 28.415301%\n",
      "> Minibatch loss at step 1500: 2.222548\n",
      "> Training f1: 13.636364%\n",
      "> Validation f1: 20.224719%\n",
      "> Minibatch loss at step 2000: 1.932306\n",
      "> Training f1: 5.405405%\n",
      "> Validation f1: 14.607948%\n",
      "> Minibatch loss at step 2500: 1.769115\n",
      "> Training f1: 11.764706%\n",
      "> Validation f1: 7.932692%\n",
      "> Minibatch loss at step 3000: 1.697427\n",
      "> Training f1: 0.000000%\n",
      "> Validation f1: 2.827763%\n",
      "> Minibatch loss at step 3500: 1.579180\n",
      "> Training f1: 8.695652%\n",
      "> Validation f1: 1.045752%\n",
      "> Minibatch loss at step 4000: 1.615987\n",
      "> Training f1: 0.000000%\n",
      "> Validation f1: 0.000000%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-2bd550e28e6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Holdout validation on TRAINING-SET for [learning rate, L2_reg] ...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mbest_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mholdout_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_of_learning_rates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_of_L2_regs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mbest_learning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-76-e5d0d9396d94>\u001b[0m in \u001b[0;36mholdout_validation\u001b[0;34m(set_of_learning_rates, set_of_regs, X_train, y_train)\u001b[0m\n\u001b[1;32m     31\u001b[0m                     \u001b[0mlearning_rate\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                 }\n\u001b[0;32m---> 33\u001b[0;31m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_predictions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m500\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/albertomariopirovano/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 766\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    767\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/albertomariopirovano/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 951\u001b[0;31m     \u001b[0mfetch_handler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FetchHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m     \u001b[0;31m# Run request and get response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/albertomariopirovano/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, graph, fetches, feeds)\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m       \u001b[0;31m# Remember the fetch if it is for a tensor handle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'GetSessionHandle'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_handles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfetch_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_fetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "set_of_learning_rates = np.arange(0.001, 0.04, step=0.001)\n",
    "set_of_L2_regs = np.arange(0.001, 0.04, step=0.001)\n",
    "\n",
    "print(\"Holdout validation on TRAINING-SET for [learning rate, L2_reg] ...\")\n",
    "\n",
    "best_params = holdout_validation(set_of_learning_rates, set_of_L2_regs, train_dataset, train_labels)\n",
    "\n",
    "best_learning_rate = best_params[0]\n",
    "best_L2_reg = best_params[1]\n",
    "\n",
    "print(\"Best parameters: [\" + str(best_learning_rate) + \", \" + str(best_L2_reg) + \"] ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 - Prediction using optimal hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for TEST-SET using best parameters ...\n",
      "Combination: [Parameters: 0.031, 0.021 | Train model]\n",
      "> Minibatch loss at step 0: 17.330044\n",
      "> Training f1: 40.9%\n",
      "> Validation f1: 28.2%\n",
      "> Minibatch loss at step 500: 1.120372\n",
      "> Training f1: 6.7%\n",
      "> Validation f1: 17.6%\n",
      "> Minibatch loss at step 1000: 0.753213\n",
      "> Training f1: 34.5%\n",
      "> Validation f1: 24.5%\n",
      "> Minibatch loss at step 1500: 0.646716\n",
      "> Training f1: 25.6%\n",
      "> Validation f1: 29.6%\n",
      "> Minibatch loss at step 2000: 0.624959\n",
      "> Training f1: 11.8%\n",
      "> Validation f1: 24.7%\n",
      "> Minibatch loss at step 2500: 0.541762\n",
      "> Training f1: 22.2%\n",
      "> Validation f1: 25.5%\n",
      "> Minibatch loss at step 3000: 0.514815\n",
      "> Training f1: 21.1%\n",
      "> Validation f1: 30.2%\n",
      "> Minibatch loss at step 3500: 0.445434\n",
      "> Training f1: 40.0%\n",
      "> Validation f1: 28.7%\n",
      "> Minibatch loss at step 4000: 0.527530\n",
      "> Training f1: 17.6%\n",
      "> Validation f1: 29.4%\n",
      "> Minibatch loss at step 4500: 0.508085\n",
      "> Training f1: 35.9%\n",
      "> Validation f1: 31.7%\n",
      "> Minibatch loss at step 5000: 0.556805\n",
      "> Training f1: 25.5%\n",
      "> Validation f1: 33.0%\n",
      "|> Test f1: 33.008130%\n",
      "|> Test accuracy: 79.568559%\n",
      "[1 0 0 ..., 0 0 0]\n",
      "[1 1 0 ..., 1 0 0]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABIQAAAJCCAYAAACxsxylAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAQGcUhAAAAAA68/+pFHOpMNGTyAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11953a748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Prediction for TEST-SET using best parameters ...\")\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    \n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Combination: [Parameters: \" +  str(best_learning_rate) + \", \" + str(best_L2_reg) + \" | Train model]\")\n",
    "    for step in np.arange(num_steps):\n",
    "                        \n",
    "        offset = (step * batch_size) % (num_examples - batch_size)\n",
    "        X_batch = train_dataset[offset:(offset + batch_size), :]\n",
    "        t_batch = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {\n",
    "            X : X_batch,\n",
    "            t : t_batch,\n",
    "            L2_reg : best_L2_reg,\n",
    "            learning_rate : best_learning_rate\n",
    "        }\n",
    "        \n",
    "        _, l, pred_batch = session.run( [optimizer, loss, train_predictions], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"> Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"> Training f1: %.1f%%\" % score(pred_batch, t_batch))\n",
    "            print(\"> Validation f1: %.1f%%\" % score(val_predictions.eval(), val_labels))\n",
    "                \n",
    "    f1_test = score(test_predictions.eval(), test_labels)\n",
    "    accuracy_test = accuracy(test_predictions.eval(), test_labels)\n",
    "    print(\"|> Test f1: %f%%\" % f1_test)\n",
    "    print(\"|> Test accuracy: %f%%\" % accuracy_test)\n",
    "    plot_results(test_predictions.eval(), test_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
