{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "import pylab as pl\n",
    "from matplotlib import collections  as mc\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (20.0, 10) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22851, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29.479667</td>\n",
       "      <td>5.668109</td>\n",
       "      <td>-0.209441</td>\n",
       "      <td>-1.345416</td>\n",
       "      <td>-2.547592</td>\n",
       "      <td>-0.433499</td>\n",
       "      <td>-0.295371</td>\n",
       "      <td>-1.271754</td>\n",
       "      <td>-1.085988</td>\n",
       "      <td>-1.853838</td>\n",
       "      <td>-0.164936</td>\n",
       "      <td>0.076310</td>\n",
       "      <td>-0.171494</td>\n",
       "      <td>-0.263780</td>\n",
       "      <td>0.252866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-8.119109</td>\n",
       "      <td>-0.918377</td>\n",
       "      <td>3.104810</td>\n",
       "      <td>-0.575573</td>\n",
       "      <td>-2.197244</td>\n",
       "      <td>-0.918233</td>\n",
       "      <td>-3.359054</td>\n",
       "      <td>1.598714</td>\n",
       "      <td>-1.490960</td>\n",
       "      <td>0.341960</td>\n",
       "      <td>1.479482</td>\n",
       "      <td>0.113068</td>\n",
       "      <td>-0.044529</td>\n",
       "      <td>-1.130631</td>\n",
       "      <td>0.348799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-12.897545</td>\n",
       "      <td>8.760454</td>\n",
       "      <td>2.352287</td>\n",
       "      <td>-0.329169</td>\n",
       "      <td>0.227234</td>\n",
       "      <td>-0.455962</td>\n",
       "      <td>-3.328908</td>\n",
       "      <td>0.392441</td>\n",
       "      <td>2.903130</td>\n",
       "      <td>-1.738854</td>\n",
       "      <td>0.510227</td>\n",
       "      <td>-1.106186</td>\n",
       "      <td>0.176982</td>\n",
       "      <td>0.568320</td>\n",
       "      <td>-1.206535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.210958</td>\n",
       "      <td>-1.071009</td>\n",
       "      <td>1.016989</td>\n",
       "      <td>-0.326063</td>\n",
       "      <td>-0.583567</td>\n",
       "      <td>-0.108464</td>\n",
       "      <td>-0.159468</td>\n",
       "      <td>-0.904758</td>\n",
       "      <td>1.466309</td>\n",
       "      <td>0.394476</td>\n",
       "      <td>0.082506</td>\n",
       "      <td>-0.034436</td>\n",
       "      <td>-0.178566</td>\n",
       "      <td>-1.071025</td>\n",
       "      <td>-0.518600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21.015684</td>\n",
       "      <td>-0.687015</td>\n",
       "      <td>-0.105652</td>\n",
       "      <td>-1.678955</td>\n",
       "      <td>2.238427</td>\n",
       "      <td>-2.418710</td>\n",
       "      <td>-3.161304</td>\n",
       "      <td>-0.056711</td>\n",
       "      <td>1.178904</td>\n",
       "      <td>2.322390</td>\n",
       "      <td>2.085181</td>\n",
       "      <td>2.039296</td>\n",
       "      <td>-1.051172</td>\n",
       "      <td>0.075783</td>\n",
       "      <td>-0.014417</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6  \\\n",
       "0  29.479667  5.668109 -0.209441 -1.345416 -2.547592 -0.433499 -0.295371   \n",
       "1  -8.119109 -0.918377  3.104810 -0.575573 -2.197244 -0.918233 -3.359054   \n",
       "2 -12.897545  8.760454  2.352287 -0.329169  0.227234 -0.455962 -3.328908   \n",
       "3   3.210958 -1.071009  1.016989 -0.326063 -0.583567 -0.108464 -0.159468   \n",
       "4  21.015684 -0.687015 -0.105652 -1.678955  2.238427 -2.418710 -3.161304   \n",
       "\n",
       "          7         8         9        10        11        12        13  \\\n",
       "0 -1.271754 -1.085988 -1.853838 -0.164936  0.076310 -0.171494 -0.263780   \n",
       "1  1.598714 -1.490960  0.341960  1.479482  0.113068 -0.044529 -1.130631   \n",
       "2  0.392441  2.903130 -1.738854  0.510227 -1.106186  0.176982  0.568320   \n",
       "3 -0.904758  1.466309  0.394476  0.082506 -0.034436 -0.178566 -1.071025   \n",
       "4 -0.056711  1.178904  2.322390  2.085181  2.039296 -1.051172  0.075783   \n",
       "\n",
       "         14  \n",
       "0  0.252866  \n",
       "1  0.348799  \n",
       "2 -1.206535  \n",
       "3 -0.518600  \n",
       "4 -0.014417  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data_1 = pd.read_csv(\"../common/albertom/train_test_split/X_train_pca.csv\", index_col=\"Unnamed: 0\")\n",
    "df_data_1 = df_data_1\n",
    "print(df_data_1.shape)\n",
    "df_data_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22851, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>34188358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>41229741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>43708758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>43058589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>25456031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0         1\n",
       "0  0  34188358\n",
       "1  0  41229741\n",
       "2  0  43708758\n",
       "3  0  43058589\n",
       "4  1  25456031"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_target_data_1 = pd.read_csv(\"../common/albertom/train_test_split/y_train.csv\", index_col=\"Unnamed: 0\")\n",
    "df_target_data_1 = df_target_data_1\n",
    "print(df_target_data_1.shape)\n",
    "df_target_data_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4033, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.718190</td>\n",
       "      <td>-6.316477</td>\n",
       "      <td>-2.083559</td>\n",
       "      <td>0.975329</td>\n",
       "      <td>-0.296123</td>\n",
       "      <td>1.974713</td>\n",
       "      <td>0.379124</td>\n",
       "      <td>-0.007346</td>\n",
       "      <td>1.958610</td>\n",
       "      <td>-1.165546</td>\n",
       "      <td>0.759263</td>\n",
       "      <td>-0.382959</td>\n",
       "      <td>-0.584112</td>\n",
       "      <td>1.553049</td>\n",
       "      <td>0.568266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.020365</td>\n",
       "      <td>-0.602705</td>\n",
       "      <td>9.139554</td>\n",
       "      <td>0.840974</td>\n",
       "      <td>0.546386</td>\n",
       "      <td>-5.230693</td>\n",
       "      <td>-0.084946</td>\n",
       "      <td>-10.231455</td>\n",
       "      <td>-0.852961</td>\n",
       "      <td>0.451181</td>\n",
       "      <td>0.669960</td>\n",
       "      <td>-0.829125</td>\n",
       "      <td>1.024185</td>\n",
       "      <td>0.229763</td>\n",
       "      <td>0.039209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.975737</td>\n",
       "      <td>-3.348945</td>\n",
       "      <td>-0.572407</td>\n",
       "      <td>0.087099</td>\n",
       "      <td>0.788437</td>\n",
       "      <td>0.297061</td>\n",
       "      <td>0.889188</td>\n",
       "      <td>0.581228</td>\n",
       "      <td>-0.122152</td>\n",
       "      <td>-0.471265</td>\n",
       "      <td>0.488114</td>\n",
       "      <td>-0.081485</td>\n",
       "      <td>-0.171996</td>\n",
       "      <td>-0.577770</td>\n",
       "      <td>-0.305709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-22.105474</td>\n",
       "      <td>0.258111</td>\n",
       "      <td>0.720882</td>\n",
       "      <td>-3.436382</td>\n",
       "      <td>-1.524025</td>\n",
       "      <td>0.087214</td>\n",
       "      <td>-0.444141</td>\n",
       "      <td>0.608868</td>\n",
       "      <td>1.049485</td>\n",
       "      <td>0.409754</td>\n",
       "      <td>-0.329480</td>\n",
       "      <td>-0.048176</td>\n",
       "      <td>-0.009099</td>\n",
       "      <td>-0.527233</td>\n",
       "      <td>1.436252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-14.011642</td>\n",
       "      <td>2.771666</td>\n",
       "      <td>-3.670775</td>\n",
       "      <td>-0.453639</td>\n",
       "      <td>-0.303950</td>\n",
       "      <td>-1.095188</td>\n",
       "      <td>-2.580395</td>\n",
       "      <td>-1.656323</td>\n",
       "      <td>-1.115220</td>\n",
       "      <td>0.806742</td>\n",
       "      <td>-0.176123</td>\n",
       "      <td>0.275889</td>\n",
       "      <td>-0.784060</td>\n",
       "      <td>1.083732</td>\n",
       "      <td>-1.022600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6  \\\n",
       "0  -1.718190 -6.316477 -2.083559  0.975329 -0.296123  1.974713  0.379124   \n",
       "1   0.020365 -0.602705  9.139554  0.840974  0.546386 -5.230693 -0.084946   \n",
       "2   9.975737 -3.348945 -0.572407  0.087099  0.788437  0.297061  0.889188   \n",
       "3 -22.105474  0.258111  0.720882 -3.436382 -1.524025  0.087214 -0.444141   \n",
       "4 -14.011642  2.771666 -3.670775 -0.453639 -0.303950 -1.095188 -2.580395   \n",
       "\n",
       "           7         8         9        10        11        12        13  \\\n",
       "0  -0.007346  1.958610 -1.165546  0.759263 -0.382959 -0.584112  1.553049   \n",
       "1 -10.231455 -0.852961  0.451181  0.669960 -0.829125  1.024185  0.229763   \n",
       "2   0.581228 -0.122152 -0.471265  0.488114 -0.081485 -0.171996 -0.577770   \n",
       "3   0.608868  1.049485  0.409754 -0.329480 -0.048176 -0.009099 -0.527233   \n",
       "4  -1.656323 -1.115220  0.806742 -0.176123  0.275889 -0.784060  1.083732   \n",
       "\n",
       "         14  \n",
       "0  0.568266  \n",
       "1  0.039209  \n",
       "2 -0.305709  \n",
       "3  1.436252  \n",
       "4 -1.022600  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data_2 = pd.read_csv(\"../common/albertom/train_test_split/X_test_pca.csv\", index_col=\"Unnamed: 0\")\n",
    "df_data_2 = df_data_2\n",
    "print(df_data_2.shape)\n",
    "df_data_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4033, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10930294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>55016784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>48129536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>37805023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>22798417</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0         1\n",
       "0  1  10930294\n",
       "1  1  55016784\n",
       "2  0  48129536\n",
       "3  1  37805023\n",
       "4  0  22798417"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_target_data_2 = pd.read_csv(\"../common/albertom/train_test_split/y_test.csv\", index_col=\"Unnamed: 0\")\n",
    "df_target_data_2 = df_target_data_2\n",
    "print(df_target_data_2.shape)\n",
    "df_target_data_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Given a set of model parameters (learning rate, regularization penalty coefficient), find the optimal parameters with cross validation\n",
    "\n",
    "def cross_validation(set_of_learning_rates, set_of_regs, X_t, y_t):\n",
    "    n_folds = 3\n",
    "    skf = StratifiedKFold(y_t, n_folds=n_folds) # Get train-validation set stratified (keeps the same distribution) splitter\n",
    "    set_of_params = [(x,y) for x in set_of_learning_rates for y in set_of_regs]\n",
    "    \n",
    "    print(\"Number of folds: \" + str(n_folds))\n",
    "    print(\"Number of parameters combinations: \" + str(len(set_of_params)))\n",
    "\n",
    "    f1 = np.zeros(len(set_of_params))\n",
    "\n",
    "    # Repeat for every train-validate combination \n",
    "    for cv_train_index, cv_val_index in skf:\n",
    "        X_cv_train, X_cv_val = X_t[cv_train_index], X_t[cv_val_index]\n",
    "        y_cv_train, y_cv_val = y_t[cv_train_index], y_t[cv_val_index]\n",
    "\n",
    "        for param_idx, param in enumerate(set_of_params):\n",
    "            print(\"Combination \" + str(param_idx) + \": Parameters: \" +  str(param))\n",
    "            print(\"Combination \" + str(param_idx) + \": Train model\" )\n",
    "            W, b, W2, b2 = train_nn(X_cv_train, y_cv_train, param[0], param[1]) # Train\n",
    "            print(\"Combination \" + str(param_idx) + \": Predict\") \n",
    "            _, f1_cv= predict(X_cv_val, y_cv_val, W, b, W2, b2) # Predict for valiation set\n",
    "            f1[param_idx] = f1[param_idx] + f1_cv # Calculate error (cross entropy)\n",
    "\n",
    "    f1 = f1 / n_folds # Calculate average error\n",
    "    \n",
    "\n",
    "    print(\"BEST LEARNING RATE (F1) = \", set_of_params[np.argmax(f1)][1])\n",
    "    print(\"BEST REG (F1) = \", set_of_params[np.argmax(f1)][0])\n",
    "    \n",
    "    # Plot the results\n",
    "    #fig = plt.figure()\n",
    "    #ax = fig.add_subplot(111, projection='3d')\n",
    "    #Y = map(lambda x: x[0], set_of_params)\n",
    "    #X = map(lambda x: x[1], set_of_params)\n",
    "    #for i in np.arange(len(accuracy)):\n",
    "    #    ax.scatter(X[i], Y[i], accuracy[i], color = \"r\")\n",
    "    #    ax.plot_surface(X[i], Y[i], accuracy[i], color = \"b\")\n",
    "    #ax.set_xlabel('alpha')\n",
    "    #ax.set_ylabel('L1')\n",
    "    #ax.set_zlabel('R2')\n",
    "    #plt.show()\n",
    "    \n",
    "    return set_of_params[np.argmax(f1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Classify the X input set using 2-layer network with weight and bias parameters for each layer as parameters (W,b and W2, b2)\n",
    "\n",
    "def predict(X, y, W, b, W2, b2):\n",
    "    hidden_layer = np.maximum(0, np.dot(X, W) + b)\n",
    "    scores = np.dot(hidden_layer, W2) + b2\n",
    "    predicted_class = np.argmax(scores, axis=1)\n",
    "    print(' > Prediction F1: %.2f' % f1_score(y, predicted_class))\n",
    "    return predicted_class, f1_score(y, predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train the 2-layer network (X_i input features, y_i input labels)\n",
    "\n",
    "def train_nn(X_i, y_i, learning_rate, reg):\n",
    "    \n",
    "    # Parameter initilization\n",
    "    W = 0.01 * np.random.randn(D,h)\n",
    "    b = np.zeros((1,h))\n",
    "    W2 = 0.01 * np.random.randn(h,K)\n",
    "    b2 = np.zeros((1,K))\n",
    "    num_examples = X_i.shape[0]\n",
    "    \n",
    "    # Epochs\n",
    "    for i in np.arange(50000):\n",
    "\n",
    "        hidden_layer = np.maximum(0, np.dot(X_i, W) + b)\n",
    "        scores = np.dot(hidden_layer, W2) + b2 # Logits\n",
    "\n",
    "        exp_scores = np.exp(scores)\n",
    "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # Turn logits into probabilities\n",
    "\n",
    "        corect_logprobs = -np.log(probs[range(num_examples),y_i])\n",
    "        data_loss = np.sum(corect_logprobs)/num_examples\n",
    "        reg_loss = 0.5*reg*np.sum(W*W) + 0.5*reg*np.sum(W2*W2) # Apply regularization\n",
    "        loss = data_loss + reg_loss # Calculate loss\n",
    "\n",
    "        # Backpropagation\n",
    "        dscores = probs\n",
    "        dscores[range(num_examples),y_i] -= 1\n",
    "        dscores /= num_examples\n",
    "\n",
    "        \n",
    "        dW2 = np.dot(hidden_layer.T, dscores)\n",
    "        db2 = np.sum(dscores, axis=0, keepdims=True)\n",
    "\n",
    "        dhidden = np.dot(dscores, W2.T)\n",
    "\n",
    "        dhidden[hidden_layer <= 0] = 0\n",
    "\n",
    "        dW = np.dot(X_i.T, dhidden)\n",
    "        db = np.sum(dhidden, axis=0, keepdims=True)\n",
    "\n",
    "        dW2 += reg * W2\n",
    "        dW += reg * W\n",
    "\n",
    "        # Update parameters\n",
    "        W += - learning_rate * dW\n",
    "        b += - learning_rate * db\n",
    "        W2 += - learning_rate * dW2\n",
    "        b2 += - learning_rate * db2\n",
    "\n",
    "    print(\"iteration %d: loss %f\" % (i, loss))\n",
    "    return W, b, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the final score for the model using test set\n",
    "\n",
    "def score_model(X_tr, Y_tr, X_ts, Y_ts, best_learning_rate, best_reg):\n",
    "    print(\"BEST LEARNING RATE (F1) = \" + str(best_learning_rate))\n",
    "    print(\"BEST REG (F1) = \" + str(best_reg))\n",
    "\n",
    "    # Train the model with the best hyperparameters\n",
    "    W, b, W2, b2 = train_nn(X_tr, Y_tr, best_learning_rate, best_reg) \n",
    "    \n",
    "    # Predict test data\n",
    "    predicted_class, f1_test = predict(X_ts, Y_ts, W, b, W2, b2)\n",
    "    print(\"F1 = \" +str(f1_test))\n",
    "    print(predicted_class)\n",
    "    print(Y_ts)\n",
    "    \n",
    "    # Plot the results\n",
    "    plt.scatter(np.arange(0, len(Y_ts)), predicted_class, color='b')\n",
    "    plt.scatter(np.arange(0, len(Y_ts)), Y_ts, color='r')\n",
    "\n",
    "    lines = []\n",
    "    ax = plt.axes()\n",
    "\n",
    "    for i in range(predicted_class.shape[0]):\n",
    "        lines.append([(i, predicted_class[i]), (i, Y_ts[i])])\n",
    "\n",
    "    lc = mc.LineCollection(lines, colors=\"g\", linewidths=1)\n",
    "\n",
    "    ax.add_collection(lc)\n",
    "    ax.autoscale()\n",
    "    ax.margins(0.1)\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prepare the dataset & configurations for the network\n",
    "\n",
    "X = df_data_1.values\n",
    "D = X.shape[1]\n",
    "num_examples = X.shape[0]\n",
    "K = len(df_target_data_1[\"0\"].unique())\n",
    "y = df_target_data_1[\"0\"].values\n",
    "\n",
    "step_size = 1e-04\n",
    "reg = 1e-03\n",
    "\n",
    "h = 10 # size of hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Input data shape: ', (22851, 15))\n",
      "('Target data shape', (22851,))\n",
      "('Number of features: ', 15)\n",
      "('Number of classes: ', 2)\n",
      "('Number of examples: ', 22851)\n",
      "('Number of hidden units: ', 10)\n"
     ]
    }
   ],
   "source": [
    "# Print dimensionalities\n",
    "\n",
    "print(\"Input data shape: \",X.shape)\n",
    "print(\"Target data shape\",y.shape)\n",
    "print(\"Number of features: \",D)\n",
    "print(\"Number of classes: \",K)\n",
    "print(\"Number of examples: \",num_examples)\n",
    "print(\"Number of hidden units: \",h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split the train and test datasets\n",
    "\n",
    "X_train = X\n",
    "X_test = df_data_2.values\n",
    "y_train = y\n",
    "y_test = df_target_data_2[\"0\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of folds: 3\n",
      "Number of parameters combinations: 4\n",
      "Combination 0: Parameters: (0.01, 0.01)\n",
      "Combination 0: Train model\n",
      "iteration 49999: loss 0.457966\n",
      "Combination 0: Predict\n",
      " > Prediction F1: 0.36\n",
      "Combination 1: Parameters: (0.01, 0.02)\n",
      "Combination 1: Train model\n",
      "iteration 49999: loss 0.461448\n",
      "Combination 1: Predict\n",
      " > Prediction F1: 0.33\n",
      "Combination 2: Parameters: (0.02, 0.01)\n",
      "Combination 2: Train model\n",
      "iteration 49999: loss 0.453718\n",
      "Combination 2: Predict\n",
      " > Prediction F1: 0.36\n",
      "Combination 3: Parameters: (0.02, 0.02)\n",
      "Combination 3: Train model\n",
      "iteration 49999: loss 0.463086\n",
      "Combination 3: Predict\n",
      " > Prediction F1: 0.33\n",
      "Combination 0: Parameters: (0.01, 0.01)\n",
      "Combination 0: Train model\n",
      "iteration 49999: loss 0.457019\n",
      "Combination 0: Predict\n",
      " > Prediction F1: 0.36\n",
      "Combination 1: Parameters: (0.01, 0.02)\n",
      "Combination 1: Train model\n",
      "iteration 49999: loss 0.466641\n",
      "Combination 1: Predict\n",
      " > Prediction F1: 0.34\n",
      "Combination 2: Parameters: (0.02, 0.01)\n",
      "Combination 2: Train model\n",
      "iteration 49999: loss 0.459690\n",
      "Combination 2: Predict\n",
      " > Prediction F1: 0.38\n",
      "Combination 3: Parameters: (0.02, 0.02)\n",
      "Combination 3: Train model\n",
      "iteration 49999: loss 0.464317\n",
      "Combination 3: Predict\n",
      " > Prediction F1: 0.35\n",
      "Combination 0: Parameters: (0.01, 0.01)\n",
      "Combination 0: Train model\n",
      "iteration 49999: loss 0.455062\n",
      "Combination 0: Predict\n",
      " > Prediction F1: 0.38\n",
      "Combination 1: Parameters: (0.01, 0.02)\n",
      "Combination 1: Train model\n",
      "iteration 49999: loss 0.463439\n",
      "Combination 1: Predict\n",
      " > Prediction F1: 0.36\n",
      "Combination 2: Parameters: (0.02, 0.01)\n",
      "Combination 2: Train model\n",
      "iteration 49999: loss 0.454721\n",
      "Combination 2: Predict\n",
      " > Prediction F1: 0.38\n",
      "Combination 3: Parameters: (0.02, 0.02)\n",
      "Combination 3: Train model\n",
      "iteration 49999: loss 0.464795\n",
      "Combination 3: Predict\n",
      " > Prediction F1: 0.35\n",
      "('BEST LEARNING RATE (F1) = ', 0.01)\n",
      "('BEST REG (F1) = ', 0.02)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the different parameters for the model & plot\n",
    "\n",
    "# set_of_learning_rates = np.arange(0.003, 0.013, step=0.001)\n",
    "# set_of_regs = np.arange(0.013, 0.023, step=0.001)\n",
    "\n",
    "set_of_learning_rates = np.arange(0.01, 0.03, step=0.01)\n",
    "set_of_regs = np.arange(0.01, 0.03, step=0.01)\n",
    "\n",
    "best_params = cross_validation(set_of_learning_rates, set_of_regs, X_train, y_train)\n",
    "\n",
    "best_learning_rate = best_params[1]\n",
    "best_reg = best_params[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST LEARNING RATE (F1) = 0.01\n",
      "BEST REG (F1) = 0.02\n",
      "iteration 49999: loss 0.465509\n",
      " > Prediction F1: 0.30\n",
      "F1 = 0.304054054054\n",
      "[1 0 0 ..., 0 0 0]\n",
      "[1 1 0 ..., 1 0 0]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABIQAAAJCCAYAAACxsxylAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10e041dd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-5ae2e443b4fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Calculate the final score for the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpredicted_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_learning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_reg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "# Calculate the final score for the model\n",
    "\n",
    "predicted_class, f1 = score_model(X_train, y_train, X_test, y_test, best_learning_rate, best_reg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
